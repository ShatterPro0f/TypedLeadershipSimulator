# Phase 4: LLM Integration - Completion Summary

## Overview

Phase 4 implements the **mandatory LLM backend** for the Typed Leadership Simulator. The LLM is central to core gameplay:

1. **Decision Interpretation**: Converts freeform player input into deterministic simulation parameters
2. **Narrative Generation**: Generates emergent stories/crises based on world state changes
3. **Ambient Dialogue**: Fills settlement with NPC-to-NPC conversations between player events

---

## âœ… Completed Components

### 1. LLM Provider Infrastructure (`include/LLM.h`, `src/core/LLMProviders.cpp`)

#### Abstract Base Class: `LLMProvider`
- Common interface for all providers
- Methods: `callLLM()`, `isAvailable()`, `estimateTokens()`, `calculateCostUSD()`
- Token estimation: ~1 token per 4 characters (OpenAI approximation)

#### Provider Implementations

**OpenAIProvider**
- Supports GPT-4 and GPT-3.5-turbo
- Cost tracking: Input $0.003/1K tokens, Output $0.004/1K tokens
- API key from environment: `OPENAI_API_KEY`
- HTTP request stubbed (ready for libcurl implementation)

**LocalLLaMAProvider (Ollama)**
- Connects to local Ollama server (default: `localhost:11434`)
- Zero cost (runs locally)
- Perfect for offline development
- Supports any Ollama model (Mistral, Llama 2, etc.)
- API endpoint from environment: `OLLAMA_ENDPOINT`

**OfflineFallbackProvider**
- Rule-based, deterministic, no network
- Template narratives for plausible but formulaic stories
- Prevents hallucination during LLM outages
- Always available (fallback safety net)

### 2. LLM Manager (`include/LLM.h`, `src/core/LLMProviders.cpp`)

#### Singleton Coordinator: `LLMManager`
- **Unified API**: Single interface for all LLM operations
- **Provider Factory**: Selects provider based on `LLMConfig`
- **Configuration**: 
  - Provider selection (OpenAI/LocalLLaMA/Offline)
  - Temperatures: 0.3 (decision), 0.7 (narrative) for balance
  - Timeouts: 3s (decision), 10s (narrative), 5s (NPC dialogue)

#### Key Methods
```cpp
// Synchronous interface (blocking)
LLMResponse interpretPlayerDecision(playerInput, context);
LLMResponse generateNarrative(worldStateContext);
LLMResponse generateNPCConversation(npcId1, npcId2, context);

// Asynchronous interface (non-blocking with callbacks)
void interpretPlayerDecisionAsync(input, context, callback);
void generateNarrativeAsync(context, callback);
void generateNPCConversationAsync(ids, context, callback);

// Utility methods
recordUsage(usage);              // Track tokens & cost
getTotalCostUSD();               // Cost estimation
recordLLMCall(tick, req, resp);  // Replay logging
replayLLMCall(tick, callType);   // Deterministic replay
```

#### Token Tracking & Cost Estimation
- Tracks input/completion tokens per call
- Aggregates daily/weekly usage
- Cost calculation: `(inputTokens/1000 * rate) + (completionTokens/1000 * rate)`
- Logs to `llm_usage.json` for monitoring

#### Caching System
- Configurable response caching (configurable: `enableCaching`)
- Hash-based lookup: `hashPrompt(prompt) -> cached_response`
- Avoids redundant LLM calls within ~5 minutes
- Reduces costs and improves responsiveness

#### Replay System (Determinism)
- Logs every LLM call: `(tick, callType) -> response`
- Enables byte-identical replay: same seed + inputs = same state
- Supports frame-by-frame debugging
- Stored in `replay_log.json` per save file

### 3. Request Queue System (`include/LLM.h`)

#### 3-Tier Priority Queue Architecture

**HIGH Priority: PlayerInputQueue**
- Player typed command â†’ IMMEDIATE processing
- Timeout: 3 seconds (player expects quick feedback)
- Blocks NPC conversations while processing
- Example: "allocate food to farmers"

**MEDIUM Priority: WorldStateNarrativeQueue**
- Triggered when world state significantly changes
- Timeout: 10 seconds (can be slower, non-blocking)
- If new request arrives while one pending â†’ drops old, queues new
- Example: Food scarcity detected, generate crisis narrative

**LOW Priority: NPCConversationQueue**
- Ambient NPC-to-NPC dialogue generation
- Timeout: 5 seconds
- Max 3 concurrent (can run in parallel)
- Only processes when world state LLM idle >10 seconds

#### Request Structure
```cpp
struct LLMRequest {
    int requestId;
    std::vector<int> npcIds;           // Affected NPCs
    std::string prompt;
    std::string callType;              // Type of request
    QueuePriority priority;
    std::function<void(const LLMResponse&)> callback;  // Callback on completion
    bool isFromReplay;                 // For deterministic replay
};
```

#### Processing Strategy
- Non-blocking: all LLM calls async; frame continues rendering
- Callback system: when LLM completes, invoke callback to update NPC state
- Fallback: if timeout, use offline fallback provider
- Error handling: retries with exponential backoff (1s, 2s, 4s, 8s)

### 4. World State Monitoring (`include/WorldState.h`, `src/core/WorldState.cpp`)

#### Continuous State Tracking: `WorldState`
- Tracks complete settlement state every tick
- Monitors NPCs, factions, resources, events for changes
- Calculates settlement-level metrics (morale, stability)

#### Change Detection with Thresholds
```
- NPC mood delta > 0.2 (20% change)
- NPC loyalty delta > 0.15 (15% change)
- Faction loyalty delta > 0.15
- Resource crossed scarcity threshold
- Event triggered
- Immigration/emigration occurred
```

#### `WorldStateChangeDetector`
- Evaluates if changes warrant LLM narrative call
- Dynamic threshold adjustment based on game state:
  - **Crisis** (stability < 0.3): Relaxed thresholds to reduce spam
  - **Peace** (stability > 0.7): Tightened thresholds for more narrative
- Debouncing: min 100 ticks between narrative calls
- Returns human-readable reason for detection

#### `ContinuousWorldStateMonitor`
- Tick-by-tick monitoring of world state
- Returns true if should trigger LLM narrative generation
- Tracks statistics:
  - Total ticks monitored
  - Total snapshots triggered
  - Average ticks per trigger

#### `BatchChangeAccumulator`
- Aggregates rapid multi-system changes
- Prevents redundant LLM calls when multiple systems trigger simultaneously
- Example: Food scarcity + 3 NPCs mood change + faction conflict â†’ ONE LLM call

#### World State Snapshots
```cpp
struct NPCSnapshot { mood, loyalty, previousMood, previousLoyalty, ... }
struct FactionSnapshot { averageLoyalty, strength, memberCount, ... }
struct ResourceSnapshot { quantity, productionRate, consumptionRate, ... }
struct EventSnapshot { name, type, impactLevel, affectedNPCIds, ... }
```

---

## ðŸ”§ Implementation Details

### Prompt Construction Helpers

```cpp
// Build world state context for LLM
std::string buildWorldStateContext(const WorldStateSnapshot& snapshot);

// Build decision interpretation prompt
// Input: "allocate food to farmers"
// Output: Structured JSON with action, tone, target_type, priority
std::string buildDecisionInterpretationPrompt(const std::string& playerInput, 
                                             const std::string& worldContext);

// Build narrative generation prompt
// Input: Current world state changes
// Output: 2-3 emergent crises/opportunities
std::string buildNarrativeGenerationPrompt(const WorldStateSnapshot& snapshot);

// Build NPC conversation prompt
// Input: Two NPC IDs, topic, location
// Output: Natural dialogue between NPCs
std::string buildNPCConversationPrompt(int npcId1, int npcId2, 
                                      const std::string& topic, 
                                      const std::string& location);
```

### Configuration Loading

```cpp
// From environment variables (first priority)
LLMConfig config = LLMConfig::loadFromEnvironment();
// OLLAMA_ENDPOINT, OPENAI_API_KEY

// From config file (fallback)
LLMConfig config = LLMConfig::loadFromFile("llm_config.json");
```

### Error Handling & Fallback Cascade

1. **Tier 1: Timeout** (LLM doesn't respond)
   - Use cached response if available
   - Fallback to offline provider

2. **Tier 2: API Error** (LLM API returns error)
   - Retry with exponential backoff (3 attempts)
   - After 3 retries: fallback to rule-based

3. **Tier 3: Invalid Response** (unparseable)
   - Extract via regex/heuristics
   - Use default parameters if extraction fails

4. **Tier 4: Complete Failure** (multiple timeouts)
   - Switch to offline fallback for 5 minutes
   - Log warning, continue game

---

## ðŸ“Š Usage Statistics & Cost Tracking

### Cost Estimation (OpenAI)
- GPT-3.5-turbo: ~$0.002 per call (150 input + 100 output tokens)
- Per 1000 NPCs, if 10 narrative calls/day: ~$0.02/day
- Per year: ~$7-10

### Token Tracking
- Input tokens: prompt + context
- Completion tokens: LLM response
- Stored in `llm_usage.json`:
  ```json
  [
    {"inputTokens": 150, "completionTokens": 100, "costUSD": 0.0019, "callType": "narrative_generation", "timestamp": "2024-01-15T10:30:00Z"},
    ...
  ]
  ```

### Performance Metrics
- Decision interpretation: 3s timeout (player expects feedback)
- Narrative generation: 10s timeout (background, can be slower)
- NPC dialogue: 5s timeout (ambient, lowest priority)
- Debouncing: min 100 ticks between narrative calls = ~1.6 seconds minimum (at 60 FPS)

---

## ðŸŽ¯ Integration Workflow

### Main Game Loop Integration
```
Each Tick:
  1. Update NPC positions & emotions
  2. Check: Is any NPC within proximity to player?
     â†’ Initiate dialogue if problem severity > threshold
  3. Update world state snapshot
  4. Check: Has world state significantly changed?
     â†’ [Async] Call LLM for narrative generation
  5. Check: Should generate NPC-to-NPC conversations?
     â†’ [Async] Call LLM for ambient dialogue
  6. Render frame
```

### Player Decision Flow
```
Player Types: "allocate food to farmers"
  â†“
[Async, HIGH priority] interpretPlayerDecisionAsync()
  â†’ LLM converts to: {action: "allocate", tone: "positive", target: "farmers"}
  â†’ Deterministic simulation updates loyalty, resources, faction state
  â†’ Display result: "Alice: 'Thank you!' Loyalty +2, Farmer faction +1"
```

### Narrative Generation Flow
```
World State Changes Detected:
  - Food crossed scarcity (120 â†’ 80)
  - Alice mood dropped (0.6 â†’ 0.3)
  â†“
[Async, MEDIUM priority] generateNarrativeAsync()
  â†’ LLM analyzes: "Settlement faces food crisis, morale declining"
  â†’ LLM generates: "Farmers report starvation. Consider rationing or hunting."
  â†’ Queue narrative issue for player to respond to
```

### Ambient Dialogue Flow
```
Two NPCs (Alice farmer, Bob merchant) near each other:
  â†“
[Async, LOW priority] generateNPCConversationAsync(alice_id, bob_id)
  â†’ LLM generates: Alice: "Did you hear? Food stores are running low."
                   Bob: "Trade caravan arriving soon. Maybe they have supplies."
  â†“
Store in conversation buffer (max 100 recent)
Player can eavesdrop to understand NPC concerns
```

---

## ðŸ§ª Testing & Validation

### Determinism Testing
- Run save with seed=42 twice
- Compare world state at ticks 1000, 5000, 10000
- Assert byte-identical output (within float64 precision)

### LLM Fallback Testing
- Block LLM API; verify offline provider activates
- Verify no hallucination (text stays within templates)
- Check determinism of offline fallback

### Performance Testing
- Memory: Peak <200MB for active set
- Frame time: <16ms per tick (60 FPS) with 1000 NPCs
- Save/load: <2 seconds for 1000 NPCs

### Token Usage Testing
- Track costs per game day
- Verify no excessive spam
- Monitor cache hit rate

---

## ðŸ“ Configuration Files

### `llm_config.json` (Optional, human-editable)
```json
{
  "provider": "LOCAL_LLAMA",
  "apiEndpoint": "http://localhost:11434",
  "apiKey": "${OPENAI_API_KEY}",
  "timeoutSeconds": 10,
  "maxRetries": 3,
  "temperatureDecisionInterpretation": 0.3,
  "temperatureNarrativeGeneration": 0.7,
  "enableTokenTracking": true,
  "enableCaching": true
}
```

### Environment Variables (Highest Priority)
```bash
export OLLAMA_ENDPOINT=http://localhost:11434
export OPENAI_API_KEY=sk-...
```

---

## ðŸ“‚ File Structure

```
include/
  LLM.h                 - All interfaces, structs, manager
  
src/core/
  LLMProviders.cpp      - Provider implementations
  WorldState.h          - World state tracking
  WorldState.cpp        - Implementation
  
data/
  (Optional) llm_config.json    - Configuration file
  
(Generated per save)
  llm_usage.json        - Token tracking log
  replay_log.json       - LLM call log for replay
```

---

## ðŸš€ Future Work

### HTTP Implementation
- Integrate libcurl for actual OpenAI API calls
- Implement Ollama HTTP client
- SSL/TLS support for production

### Advanced Features
- Model selection UI (GPT-4 vs 3.5 vs Local)
- Cost monitoring dashboard
- Token budget enforcement
- Prompt optimization (shorter context for faster responses)

### Performance Optimization
- Batch LLM requests (send multiple at once)
- Streaming responses for faster perceived response time
- Client-side LLM embedding for semantic search
- Context pruning for 1000+ NPCs

### Evaluation & QA
- Automated testing of LLM outputs
- Sensitivity analysis: how do temperature changes affect gameplay?
- User study: do narrative issues feel natural and emergent?

---

## âœ¨ Key Design Principles

1. **LLM is Mandatory**: Not optionalâ€”core to gameplay from day 1
2. **Deterministic Cascade**: LLM generates narrative; simulation handles consequences (seeded RNG)
3. **Offline-First**: Default to LocalLLaMA (Ollama) for independent development
4. **Non-Blocking**: All LLM calls async; game loop never stalls
5. **Error Resilient**: Multi-tier fallback ensures game always playable
6. **Cost-Conscious**: Token tracking, caching, throttling prevent bill shock
7. **Reproducible**: Replay logging enables exact deterministic replay with LLM call logs

---

## ðŸŽ® Player Experience

### What Players See
1. **Dynamic Crises**: World state changes trigger emergent narrative issues
2. **Reactive NPCs**: Nearby NPCs approach with problems; player responds with typed commands
3. **Living Settlement**: Ambient NPC conversations give settlement life
4. **Natural Language**: Player can type commands in plain English; LLM converts to simulation

### What Players Don't See (But Happens Behind Scenes)
1. Continuous world state monitoring
2. Async LLM calls in background
3. Deterministic consequence calculations
4. Token tracking & cost monitoring
5. Replay logging for debugging

---

## ðŸ Next Steps

1. **Integrate into Main Game Loop**: Connect LLM callbacks to simulation state updates
2. **UI Integration**: Display LLM-generated narratives in dialogue system
3. **Testing**: Run comprehensive QA on LLM outputs and integration
4. **Performance Tuning**: Optimize for 1000+ NPCs without LLM spam
5. **Documentation**: Create player-facing help system for LLM-interpreted commands

---

**Status**: Phase 4 LLM Infrastructure âœ… COMPLETE  
**Next Phase**: Phase 5 - Main Loop Integration & UI  
**Estimated Time**: Complete before next major milestone
